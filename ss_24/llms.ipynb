{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06c13e8-4c8d-4a3d-8da2-0a654e3037fd",
   "metadata": {},
   "source": [
    "# 0. Install required libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484b829-9bfd-482b-b597-02ce2ff586e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wheel packaging ninja pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d2b652-a302-4a59-9817-3dc721d18f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efbea9-3c36-4b19-9820-1c41136001e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/cluster/user/nimeseth/.cache\n",
    "%env HF_DATASETS_CACHE=/cluster/user/nimeseth/datasets\n",
    "%env TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04130b-eb54-46ce-ae94-9b1416060812",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62b9e8-f8ea-4eb3-a03a-7444312dbefb",
   "metadata": {},
   "source": [
    "# 1. Download and install LLM (Gemma-2B / Gemma 7B / Phi-3) and setup the model\n",
    "Change the variable `model_id` to change the model to download and run in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef6c77-7dca-4781-9f84-c1e08e30fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "#model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "model_id = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "#model_id = \"google/gemma-7b-it\"\n",
    "#model_id = \"google/gemma-2b-it\"\n",
    "#model_id = \"LeoLM/leo-hessianai-13b-chat-bilingual\"\n",
    "\n",
    "access_token = \"<REPLACE_WITH_ACCESS_TOKEN>\" # Provided by instructor\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    "    token = access_token,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d13015-e5e1-49a5-a449-840b2cfbe418",
   "metadata": {},
   "source": [
    "# 2. Run inference on the model for testing purposes\n",
    "Play around with the arguments for the generation process. Setting `temperature` to a higher value will increase randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e6a04-e3d1-4e65-ac1c-26ec98a89ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.5,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of Germany is Berlin. Berlin has around 3 Million inhabitants. The most important attraction is the 'Brandenburger Tor'\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Italy?\"},\n",
    "]\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a96c7d-37d9-40d7-9885-f4d1bad36740",
   "metadata": {},
   "source": [
    "# 3. Load transcripts from folder and perform the same action for each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e14cee-7d7e-4c07-8c44-0d526f4ac337",
   "metadata": {},
   "source": [
    "Set the generation parameters for the following transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d77789-fb4f-4fb9-b84f-e16e9d4fe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.2,\n",
    "    \"do_sample\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99dd28-ea5d-4741-8ffb-08c0e0a33efc",
   "metadata": {},
   "source": [
    "Define a folder to save the LLM's output for each transcript to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4239245-d78d-45be-81d3-43608de15c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_OUTPUT_FOLDER = \"llm_output\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(LLM_OUTPUT_FOLDER):\n",
    "    os.makedirs(LLM_OUTPUT_FOLDER)\n",
    "    print(f\"Folder '{LLM_OUTPUT_FOLDER}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{LLM_OUTPUT_FOLDER}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d56d99-3763-4b88-ae0f-18346708b412",
   "metadata": {},
   "source": [
    "Create a function that takes a transcript and creates a prompt object for the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa5696-2a8d-4dcb-ad19-ac5e00eb1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_messages(prompt, transcript):\n",
    "    prompt_with_input = f\"{prompt}\\n\\nHere is the transcript: '{transcript.strip()}'\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_with_input}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f8875-75eb-4ce4-8e8d-7e21ebb36f4b",
   "metadata": {},
   "source": [
    "Define a function to write LLM results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98835d8b-4fcb-45e1-8930-8937a3666c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_result_to_file(llm_results, output_file_name):\n",
    "    \n",
    "    fields = [\"yt_id\", \"transcript\", \"llm_result\"]\n",
    "\n",
    "    csv_file = f\"{LLM_OUTPUT_FOLDER}/{output_file_name}\" \n",
    "    \n",
    "    # Write data to the CSV file\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fields, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "        \n",
    "        # Write the header row\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the data rows\n",
    "        for row in llm_results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b700794-e6b2-4b22-a2bf-b97cf9f4babe",
   "metadata": {},
   "source": [
    "Iterate through all files in the transcript folder and process with LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaf8cd-a9e3-4d3a-be06-20c6fef6063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"transcripts\"\n",
    "\n",
    "# Get a list of all text files in the folder\n",
    "transcript_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "\n",
    "rows = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Iterate through the list of text files\n",
    "for file_path in transcript_files:\n",
    "\n",
    "    counter += 1\n",
    "    if counter > 5:\n",
    "        break\n",
    "        \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        # Read the content of the file and append it to the variable\n",
    "        transcript = file.read()\n",
    "        print(f\"{counter} File: {file_path} with length {len(transcript)}\")\n",
    "\n",
    "        # Create the messages object using the previously defined function\n",
    "        messages = create_prompt_messages(\"List five words that capture the content of the video! Return only the five words! All words must be in GERMAN! Do not number the words!\", transcript)\n",
    "\n",
    "        try:\n",
    "            output = pipe(messages, **generation_args)\n",
    "            llm_result = output[0]['generated_text']\n",
    "            row = { \"yt_id\" : file_path.replace(\".txt\", \"\"), \"transcript\": transcript, \"llm_result\": llm_result }\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "write_result_to_file(rows, \"llm_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778218e5-1270-42a3-b1e2-f35d4e57d004",
   "metadata": {},
   "source": [
    "## Estimating the tokens in a given text\n",
    "Helpful tool to check how many tokens a text contains: [OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n",
    "\n",
    "A rule of thumbs is to devide the length in characters by 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d726f",
   "metadata": {},
   "source": [
    "# 4. Load transcripts from CSV file and run prompt against each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = 'transcripts/transcripts.csv'\n",
    "\n",
    "llm_results = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Open the CSV file and read it\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "\n",
    "        counter += 1\n",
    "        if counter > 20:\n",
    "            break\n",
    "            \n",
    "        yt_id = row['yt_id']\n",
    "        transcript = row['transcript']\n",
    "\n",
    "        print(f\"{counter} yt_id: {yt_id} with transcript length {len(transcript)}\")\n",
    "    \n",
    "        # Create the messages object using the previously defined function\n",
    "        messages = create_prompt_messages(\"List five words that capture the content of the video! Return only the five words! All words must be in GERMAN! Do not number the words!\", transcript)\n",
    "\n",
    "\n",
    "        try:\n",
    "            output = pipe(messages, **generation_args)\n",
    "            llm_result = output[0]['generated_text']\n",
    "            row = { \"yt_id\" : yt_id, \"transcript\": transcript, \"llm_result\": llm_result }\n",
    "            llm_results.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "    write_result_to_file(llm_results, f\"{LLM_OUTPUT_FOLDER}/llm_results_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702e989",
   "metadata": {},
   "source": [
    "# 5. Merge results from multiple CSV files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9654b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "input_folder = f\"{LLM_OUTPUT_FOLDER}\"\n",
    "output_file_name = \"llm_results_combined.csv\"\n",
    "\n",
    "# Get a list of all CSV files in the output directory\n",
    "all_files = glob.glob(input_folder + \"/*.csv\")\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file and read it into a DataFrame, then append to the list\n",
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(f\"{input_folder}/{output_file_name}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
